@Inproceedings {izadi11,
abstract     = {<p>KinectFusion enables a user holding and moving a standard Kinect camera to
                rapidly create detailed 3D reconstructions of an indoor scene. Only the depth
                data from Kinect is used to track the 3D pose of the sensor and reconstruct,
                geometrically precise, 3D models of the physical scene in real-time. The
                capabilities of KinectFusion, as well as the novel GPU-based pipeline are
                described in full. We show uses of the core system for low-cost handheld
                scanning, and geometry-aware augmented reality and physics-based interactions.
                Novel extensions to the core GPU pipeline demonstrate object segmentation and
                user interaction directly in front of the sensor, without degrading camera
                tracking or reconstruction. These extensions are used to enable real-time
                multi-touch interactions anywhere, allowing any planar or non-planar
                reconstructed physical surface to be appropriated for touch.</p>},
author       = {Shahram Izadi and David Kim and Otmar Hilliges and David Molyneaux and Richard
                Newcombe and Pushmeet Kohli and Jamie Shotton and Steve Hodges and Dustin Freeman
                and Andrew Davison and Andrew Fitzgibbon},
month        = {October},
publisher    = {ACM Symposium on User Interface Software and Technology},
title        = {KinectFusion: Real-time 3D Reconstruction and Interaction Using a Moving Depth
                Camera},
url          = {http://research.microsoft.com/apps/pubs/default.aspx?id=155416},
year         = {2011},
}

@Inproceedings {newcombe11,
abstract     = {<p>We present a system for accurate real-time mapping of complex and arbitrary
                indoor scenes in variable lighting conditions, using only a moving low-cost depth
                camera and commodity graphics hardware. We fuse all of the depth data streamed
                from a Kinect sensor into a single global implicit surface model of the observed
                scene in real-time. The current sensor pose is simultaneously obtained by
                tracking the live depth frame relative to the global model using a coarse-to-fine
                iterative closest point (ICP) algorithm, which uses all of the observed depth
                data available. We demonstrate the advantages of tracking against the growing
                full surface model compared with frame-to-frame tracking, obtaining tracking and
                mapping results in constant time within room sized scenes with limited drift

                and high accuracy. We also show both qualitative and quantitative results
                relating to various aspects of our tracking and mapping system. Modelling of
                natural scenes, in real-time with only commodity sensor and GPU hardware,
                promises an exciting step forward

                in augmented reality (AR), in particular, it allows dense surfaces to be
                reconstructed in real-time, with a level of detail and robustness beyond any
                solution yet presented using passive computer vision.</p>},
author       = {Richard A. Newcombe and Shahram Izadi and Otmar Hilliges and David Molyneaux and
                David Kim and Andrew J. Davison and Pushmeet Kohli and Jamie Shotton and Steve
                Hodges and Andrew Fitzgibbon},
booktitle    = {IEEE ISMAR},
month        = {October},
publisher    = {IEEE},
title        = {KinectFusion: Real-Time Dense Surface Mapping and Tracking},
url          = {http://research.microsoft.com/apps/pubs/default.aspx?id=155378},
year         = {2011},
}

@article{Whelan14,
author = {Whelan, Thomas and Kaess, Michael and Johannsson, Hordur and Fallon, Maurice and Leonard, John J. and McDonald, John},
title = {Real-time large-scale dense RGB-D SLAM with volumetric fusion},
year = {2014},
doi = {10.1177/0278364914551008},
abstract ={We present a new simultaneous localization and mapping (SLAM) system capable of producing high-quality globally consistent surface reconstructions over hundreds of meters in real time with only a low-cost commodity RGB-D sensor. By using a fused volumetric surface reconstruction we achieve a much higher quality map over what would be achieved using raw RGB-D point clouds. In this paper we highlight three key techniques associated with applying a volumetric fusion-based mapping system to the SLAM problem in real time. First, the use of a GPU-based 3D cyclical buffer trick to efficiently extend dense every-frame volumetric fusion of depth maps to function over an unbounded spatial region. Second, overcoming camera pose estimation limitations in a wide variety of environments by combining both dense geometric and photometric camera pose constraints. Third, efficiently updating the dense map according to place recognition and subsequent loop closure constraints by the use of an ‘as-rigid-as-possible’ space deformation. We present results on a wide variety of aspects of the system and show through evaluation on de facto standard RGB-D benchmarks that our system performs strongly in terms of trajectory estimation, map quality and computational performance in comparison to other state-of-the-art systems.},
URL = {http://ijr.sagepub.com/content/early/2014/12/07/0278364914551008.abstract},
eprint = {http://ijr.sagepub.com/content/early/2014/12/07/0278364914551008.full.pdf+html},
journal = {The International Journal of Robotics Research}
}